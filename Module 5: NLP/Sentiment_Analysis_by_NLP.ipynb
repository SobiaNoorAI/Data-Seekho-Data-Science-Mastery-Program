{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMg6+ukh1a0XlbvpuOgN3kx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SobiaNoorAI/Data-Seekho-Data-Science-Mastery-Program/blob/main/Module%205%3A%20NLP/Sentiment_Analysis_by_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleaning & Preprocessing"
      ],
      "metadata": {
        "id": "7T1AfPgJeFle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Unnecessary Text\n",
        "\n",
        "\n",
        "*   Non-Alphabet Characters\n",
        "*   URLs\n",
        "*   Extra Spaces\n",
        "\n"
      ],
      "metadata": {
        "id": "aRldsaibqaXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove non-alphabet characters (except spaces)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "text = \"Welcome to Wallerobot 2025. Visit us  https://www.wallerobot.com and http://sobianoor.net .  This is an   NLP  book. We sold 550 Book this year. We use special characters like !@#$%^&*()_+=-`~[]\\{}|;':\\\",./<>?\"\n",
        "cleaned_text = clean_text(text)\n",
        "cleaned_text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "MXoLgktarWvi",
        "outputId": "a9115072-8438-40b1-d89f-4800d7a58678"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'welcome to wallerobot visit us and this is an nlp book we sold book this year we use special characters like'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "_qX_FOWWo1kZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "splits the text into smaller units called tokens"
      ],
      "metadata": {
        "id": "vu5R4NH0sgXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import word tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# apply\n",
        "tokens = word_tokenize(cleaned_text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViwU4gosny2d",
        "outputId": "152eb87c-711b-41f2-accf-2e606abb3fe2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['welcome', 'to', 'wallerobot', 'visit', 'us', 'and', 'this', 'is', 'an', 'nlp', 'book', 'we', 'sold', 'book', 'this', 'year', 'we', 'use', 'special', 'characters', 'like']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Text into Features (BoW & TF-IDF)"
      ],
      "metadata": {
        "id": "j_7M8BDCs3iE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning Models work only on numeric data"
      ],
      "metadata": {
        "id": "YzKxvQKPtDgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words (BoW)\n",
        "\n",
        "A text modeling technique that counts the frequency of each word in a document"
      ],
      "metadata": {
        "id": "EGqWFny9tcSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: apply BoW on cleaned_text\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Assuming 'cleaned_text' is already defined from the previous code\n",
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the cleaned text\n",
        "bow_representation = vectorizer.fit_transform([cleaned_text])\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Get the BoW representation as a dense array\n",
        "bow_array = bow_representation.toarray()\n",
        "\n",
        "# Print the BoW representation\n",
        "print(bow_array)\n",
        "feature_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aaxd6xLltnOv",
        "outputId": "9a0709f3-42a8-47e6-991f-b3120c5c69dc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['an', 'and', 'book', 'characters', 'is', 'like', 'nlp', 'sold',\n",
              "       'special', 'this', 'to', 'us', 'use', 'visit', 'wallerobot', 'we',\n",
              "       'welcome', 'year'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the cleaned text\n",
        "X = vectorizer.fit_transform([cleaned_text])\n",
        "\n",
        "# BoW representation as a dense array\n",
        "print(X.toarray())\n",
        "\n",
        "# Feature names (words)\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVimcbW0uS85",
        "outputId": "10500b6e-7062-43ac-aee4-759b0d4d00c0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1]]\n",
            "['an' 'and' 'book' 'characters' 'is' 'like' 'nlp' 'sold' 'special' 'this'\n",
            " 'to' 'us' 'use' 'visit' 'wallerobot' 'we' 'welcome' 'year']\n"
          ]
        }
      ]
    }
  ]
}